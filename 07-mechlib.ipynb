{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechlib / 3D models\n",
    "\n",
    "As we saw in the overview, there's no need to worry about variations between the different version. A quick examination also reveals the same file table as the sounds use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from struct import Struct, unpack_from\n",
    "\n",
    "MECHLIB_FOOTER = Struct(\"<2I\")\n",
    "MECHLIB_RECORD = Struct(\"<2I64s76x\")\n",
    "\n",
    "\n",
    "def extract_table(data):\n",
    "    offset = len(data) - MECHLIB_FOOTER.size\n",
    "    _, count = MECHLIB_FOOTER.unpack_from(data, offset)\n",
    "\n",
    "    for _ in range(count):\n",
    "        # walk the table backwards\n",
    "        offset -= MECHLIB_RECORD.size\n",
    "        # not sure what extra is\n",
    "        start, length, name = MECHLIB_RECORD.unpack_from(data, offset)\n",
    "        name = name.rstrip(b\"\\x00\").decode(\"ascii\")\n",
    "        yield name, data[start : start + length]\n",
    "\n",
    "\n",
    "data = Path(\"install/v1.0-us-post/zbd/mechlib.zbd\").read_bytes()\n",
    "\n",
    "output_path = Path.cwd() / \"models\" / \"mechlib\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "contents = {}\n",
    "for name, model in extract_table(data):\n",
    "    if name in contents:\n",
    "        print(\"duplicate:\", name)\n",
    "    contents[name] = model\n",
    "    (output_path / name).write_bytes(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `.flt` file endings, I expected the other files to be OpenFlight scenes.\n",
    "\n",
    "> OpenFlight format were rapidly adopted by the early commercial flight simulation industry in the later 80's and early 90's &mdash; https://en.wikipedia.org/wiki/OpenFlight\n",
    "\n",
    "MechWarrior 3 was released in May 1999 with DirectX 6. At that time, the latest OpenFlight specification they might have had access to is version 15.6.0 (OpenFlight 15.5.1 [was published in July 1998](https://portal.presagis.com/support/solutions/articles/19000072973-openflight-15-5-1), OpenFlight 15.4.1 was July 1998, and OpenFlight 15.0 in October 1996) . Thanks to this being an industry specification, you can [see the publication dates and still download all specifications](https://www.presagis.com/en/glossary/detail/openflight/)!\n",
    "\n",
    "Except these files don't seem to follow the OpenFlight spec. To be fair, OpenFlight does say custom binary serialisations are allowed. The first hint is OpenFlight is supposed to be big endian (the order in which bytes are written for multi-byte data types is known as [endianness](https://en.wikipedia.org/wiki/Endianness)). But most data in these files seems to be little endian. It's obvious from e.g. deserialising integers or floating point values in both big and little endian that some values make more sense range-wise in little endian. This also makes sense, as x86 processors are little endian, and in a time with limited CPU power, you'd want to avoid having to convert endianness. But even then, it isn't OpenFlight.\n",
    "\n",
    "Trying to cross-match other 3D file formats also yielded no results. It wasn't a DirectX `.x` model, Wavefront `.obj`, not `.fbx` Filmbox, and not an Autodesk `.3ds` format.\n",
    "\n",
    "---\n",
    "\n",
    "Really, the only way to do this then was to reverse engineer the model loading code from the executable, instead of reverse engineering the file format.\n",
    "\n",
    "The bad news is it's C++ code (MFC). And the compiler (Visual Studio circa 1999) produces absolutely horrible assembly. Reordered instructions, inlined functions like `memcpy`/`strcpy` et al., and interesting optimisations around calling conventions.\n",
    "\n",
    "The good news. While a lot of the debug functions are stubbed out (probably via `#ifndef DEBUG` or something), the debug strings are still present in early versions. There also seems to be dead/duplicate code for loading mesh/models, some using `ReadFileA` (seems to be used in production) and some using `fread` (maybe debug functionality, since the density of debug messages is higher).\n",
    "\n",
    "With this, I was able to reliably extract most model data. But I don't really know how to show this process. I have annotated the code in the standalone library with the debug strings I found, so it you're interested, you should be able to follow along. Hopefully I can release the Ghidra project at some point, but I'm unsure if it makes sense without the executable, and I'd rather not distribute that (although without other files, it might be OK legally speaking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to analysing `mechlib.zbd` a bit. The files `format`, `version`, and `materials` are the odd ones out. `format` and `version` are uninformative without context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0|01 00 00 00            |....\n",
      "0|1B 00 00 00            |....\n"
     ]
    }
   ],
   "source": [
    "from helpers import hexdump\n",
    "\n",
    "hexdump(contents[\"format\"])\n",
    "hexdump(contents[\"version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`materials` is a binary file that seems to map textures to models and even 'mech variants:\n",
    "\n",
    "```bash\n",
    "$ strings \"models/mechlib/materials\" | sort | uniq\n",
    "[...]\n",
    "annihilator01\n",
    "annihilator02\n",
    "annihilator03\n",
    "annihilator04\n",
    "annihilator05\n",
    "annihilator06\n",
    "annihilator07\n",
    "annihilator08\n",
    "annihilator09\n",
    "annihilator10\n",
    "annihilator11\n",
    "annihilator12\n",
    "annihilator13\n",
    "annihilator14\n",
    "annihilator15\n",
    "[...]\n",
    "$ strings \"models/mechlib/materials\" | wc -l\n",
    "     434\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (output_path / \"materials\").read_bytes()\n",
    "\n",
    "count, = unpack_from(\"<I\", data, 0)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That isn't worlds apart from the line counts. First and last 64 bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00|BA 01 00 00 FF 11 FF 7F 00 00 7F 43 00 00 7F 43|...........C...C\n",
      "16|00 00 7F 43 5C 81 61 00 00 00 00 00 00 00 00 3F|...C\\.a........?\n",
      "32|00 00 00 3F 00 00 00 00 00 00 00 00 09 00 00 00|...?............\n",
      "48|6D 65 63 68 62 61 79 30 32 FF 11 FF 7F 00 00 7F|mechbay02.......\n",
      "---\n",
      "00|61 6E 69 5F 63 30 31 FF 11 FF 7F 00 00 7F 43 00|ani_c01.......C.\n",
      "16|00 7F 43 00 00 7F 43 04 C5 61 00 00 00 00 00 00|..C...C..a......\n",
      "32|00 00 3F 00 00 00 3F 00 00 00 00 00 00 00 00 0D|..?...?.........\n",
      "48|00 00 00 6D 61 64 63 61 74 5F 63 70 69 74 30 35|...madcat_cpit05\n"
     ]
    }
   ],
   "source": [
    "from helpers import hexdump\n",
    "\n",
    "hexdump(data[:64], width=16)\n",
    "print(\"---\")\n",
    "hexdump(data[-64:], width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.86199095022624"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data) - 4) / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String length seems to be variable, hence why the record size isn't constant. We also know the name seems to come last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_from(\"<2I\", data, 4 + 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'mechbay02',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_from(\"9s\", data, 4 + 36 + 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 mechbay02\n",
      "1 9 mechbay01\n",
      "'ascii' codec can't decode byte 0xac in position 12: ordinal not in range(128) 154 b'\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\xac\\x81a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00?\\x00\\x00\\x00?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00POLE\\xff\\x11\\xff\\x7f\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\xd4\\x81a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00?\\x00\\x00\\x00?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00POLETOP\\xff\\x11\\xff\\x7f\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\x00\\x00\\x7fC\\xfc\\x81a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00?\\x00\\x00\\x00?\\x00'\n"
     ]
    }
   ],
   "source": [
    "count, = unpack_from(\"<I\", data, 0)\n",
    "offset = 4\n",
    "for i in range(count):\n",
    "    *unk, length = unpack_from(\"<40BI\", data, offset)\n",
    "    offset += 40 + 4\n",
    "    name = data[offset : offset + length]\n",
    "    try:\n",
    "        name = name.decode(\"ascii\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(e, offset, name[:128])\n",
    "        break\n",
    "    offset += length\n",
    "    print(i, length, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang. Seems like we're a few bytes out for the next record - 40 to be exact, which is uncanny since that's the exact size of the unknown record data. I'll spare you the trial and error and get straight to a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAT_HEADER_SIZE = 40\n",
    "count, = unpack_from(\"<I\", data, 0)\n",
    "offset = 4\n",
    "items = []\n",
    "for i in range(count):\n",
    "    record = data[offset : offset + MAT_HEADER_SIZE]\n",
    "    offset += MAT_HEADER_SIZE\n",
    "    length, = unpack_from(\"<I\", data, offset)\n",
    "    offset += 4\n",
    "    name = data[offset : offset + length]\n",
    "    try:\n",
    "        name = name.decode(\"ascii\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        name = None\n",
    "        offset -= 4\n",
    "    else:\n",
    "        offset += length\n",
    "    items.append((name, record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, some materials have no name. I assume this means that materials are looked up by index, and also that some materials have no texture. \n",
    "\n",
    "This could make sense if material names aren't used in the lookup, but e.g. index. But trail and error isn't a good parsing strategy. There should be a flag that indicates that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 255]\n",
      "1 [16, 17]\n",
      "2 [0, 255]\n",
      "3 [0, 127]\n",
      "4 [0]\n",
      "5 [0]\n",
      "6 [0, 69, 89, 126, 127, 238]\n",
      "7 [0, 66, 67]\n",
      "8 [0]\n",
      "9 [0]\n",
      "10 [0, 19, 31, 126, 127, 138, 238]\n",
      "11 [0, 66, 67]\n",
      "12 [0]\n",
      "13 [0]\n",
      "14 [0, 19, 69, 126, 127, 238]\n",
      "15 [0, 66, 67]\n",
      "16 [0, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 108, 116, 124, 132, 140, 148, 156, 164, 172, 180, 188, 196, 204, 212, 220, 228, 236, 244, 252]\n",
      "17 [0, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\n",
      "18 [0, 97]\n",
      "19 [0]\n",
      "20 [0]\n",
      "21 [0]\n",
      "22 [0]\n",
      "23 [0]\n",
      "24 [0]\n",
      "25 [0]\n",
      "26 [0]\n",
      "27 [63]\n",
      "28 [0]\n",
      "29 [0]\n",
      "30 [0]\n",
      "31 [63]\n",
      "32 [0]\n",
      "33 [0]\n",
      "34 [0]\n",
      "35 [0]\n",
      "36 [0]\n",
      "37 [0]\n",
      "38 [0]\n",
      "39 [0]\n"
     ]
    }
   ],
   "source": [
    "values = [set() for _ in range(MAT_HEADER_SIZE)]\n",
    "\n",
    "for name, record in items:\n",
    "    for i, r in enumerate(record):\n",
    "        values[i].add(r)\n",
    "\n",
    "for i, val in enumerate(values):\n",
    "    print(i, sorted(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can make some guesses. The last 20 bytes seem to be 4 byte/32 bit integers. Bytes 16 and 17 are extremely varied, and could be little endian integers, or maybe colour (RGB555)? Bytes 4-15 are quite regular, although I have no clue what they mean.\n",
    "\n",
    "One of the first four bytes is a good candidate for the name switch. It can't be the first, as some of the unnamed materials have the same values as named materials. It isn't the second (tried it), but I got lucky on the third:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, = unpack_from(\"<I\", data, 0)\n",
    "offset = 4\n",
    "\n",
    "mat_to_tex = []\n",
    "for i in range(count):\n",
    "    has_name = data[offset + 2] == 255\n",
    "    offset += MAT_HEADER_SIZE\n",
    "    if has_name:\n",
    "        length, = unpack_from(\"<I\", data, offset)\n",
    "        offset += 4\n",
    "        texture = data[offset : offset + length].decode(\"ascii\")\n",
    "        offset += length\n",
    "    else:\n",
    "        texture = None\n",
    "    mat_to_tex.append(texture)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"materials.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mat_to_tex, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom. It's a good start, good enough to load the textures into a 3D program. I wrote the library to export models to JSON, and then a script for [Blender](https://www.blender.org/) to create a 3D scene from the JSON. It largely works :)\n",
    "\n",
    "For the 'mechs, the \"head\" mesh (texture index 13) is just a blank rectangle - it's one of the ones without a name. So a bit more understanding of the material attributes might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up\n",
    "\n",
    "[Motion/animations](08-motion.ipynb)\n",
    "\n",
    "**WARNING**: Work in progress below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[255]\t[0, 255]\n",
      "1\t[17]\t[16]\n",
      "2\t[255]\t[0]\n",
      "3\t[127]\t[0]\n",
      "4\t[0]\t[0]\n",
      "5\t[0]\t[0]\n",
      "6\t[127]\t[0, 69, 89, 126, 127, 238]\n",
      "7\t[67]\t[0, 66, 67]\n",
      "8\t[0]\t[0]\n",
      "9\t[0]\t[0]\n",
      "10\t[127]\t[0, 19, 31, 126, 127, 138, 238]\n",
      "11\t[67]\t[0, 66, 67]\n",
      "12\t[0]\t[0]\n",
      "13\t[0]\t[0]\n",
      "14\t[127]\t[0, 19, 69, 126, 127, 238]\n",
      "15\t[67]\t[0, 66, 67]\n",
      "16\t[4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 108, 116, 124, 132, 140, 148, 156, 164, 172, 180, 188, 196, 204, 212, 220, 228, 236, 244, 252]\t[0]\n",
      "17\t[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\t[0]\n",
      "18\t[97]\t[0]\n",
      "19\t[0]\t[0]\n",
      "20\t[0]\t[0]\n",
      "21\t[0]\t[0]\n",
      "22\t[0]\t[0]\n",
      "23\t[0]\t[0]\n",
      "24\t[0]\t[0]\n",
      "25\t[0]\t[0]\n",
      "26\t[0]\t[0]\n",
      "27\t[63]\t[63]\n",
      "28\t[0]\t[0]\n",
      "29\t[0]\t[0]\n",
      "30\t[0]\t[0]\n",
      "31\t[63]\t[63]\n",
      "32\t[0]\t[0]\n",
      "33\t[0]\t[0]\n",
      "34\t[0]\t[0]\n",
      "35\t[0]\t[0]\n",
      "36\t[0]\t[0]\n",
      "37\t[0]\t[0]\n",
      "38\t[0]\t[0]\n",
      "39\t[0]\t[0]\n"
     ]
    }
   ],
   "source": [
    "named_values = [set() for _ in range(MAT_HEADER_SIZE)]\n",
    "unnamed_values = [set() for _ in range(MAT_HEADER_SIZE)]\n",
    "\n",
    "for name, record in items:\n",
    "    if name:\n",
    "        for i, r in enumerate(record):\n",
    "            named_values[i].add(r)\n",
    "    else:\n",
    "        for i, r in enumerate(record):\n",
    "            unnamed_values[i].add(r)\n",
    "\n",
    "for i, (named, unnamed) in enumerate(zip(named_values, unnamed_values)):\n",
    "    print(i, sorted(named), sorted(unnamed), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can discard the last 20 bytes, since they don't vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[255]\t[0, 255]\n",
      "1\t[17]\t[16]\n",
      "2\t[255]\t[0]\n",
      "3\t[127]\t[0]\n",
      "4\t[0]\t[0]\n",
      "5\t[0]\t[0]\n",
      "6\t[127]\t[0, 69, 89, 126, 127, 238]\n",
      "7\t[67]\t[0, 66, 67]\n",
      "8\t[0]\t[0]\n",
      "9\t[0]\t[0]\n",
      "10\t[127]\t[0, 19, 31, 126, 127, 138, 238]\n",
      "11\t[67]\t[0, 66, 67]\n",
      "12\t[0]\t[0]\n",
      "13\t[0]\t[0]\n",
      "14\t[127]\t[0, 19, 69, 126, 127, 238]\n",
      "15\t[67]\t[0, 66, 67]\n",
      "16\t[4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 108, 116, 124, 132, 140, 148, 156, 164, 172, 180, 188, 196, 204, 212, 220, 228, 236, 244, 252]\t[0]\n",
      "17\t[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\t[0]\n",
      "18\t[97]\t[0]\n",
      "19\t[0]\t[0]\n"
     ]
    }
   ],
   "source": [
    "HALF = MAT_HEADER_SIZE // 2\n",
    "named_values = [set() for _ in range(HALF)]\n",
    "unnamed_values = [set() for _ in range(HALF)]\n",
    "\n",
    "for name, record in items:\n",
    "    if name:\n",
    "        for i, r in enumerate(record[:HALF]):\n",
    "            named_values[i].add(r)\n",
    "    else:\n",
    "        for i, r in enumerate(record[:HALF]):\n",
    "            unnamed_values[i].add(r)\n",
    "\n",
    "for i, (named, unnamed) in enumerate(zip(named_values, unnamed_values)):\n",
    "    print(i, sorted(named), sorted(unnamed), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
